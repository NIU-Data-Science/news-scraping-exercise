# -*- coding: utf-8 -*-
"""
Created on Wed Sep 16 09:31:36 2020

@author: metalcorebear
"""

#News Scrape

#Import libraries
import selenium.webdriver as webdriver
import time
from bs4 import BeautifulSoup
import requests
from lxml import html
import pickle
import os
import json
from datetime import datetime
from datetime import timezone
from datetime import date as datemethod

#Define functions
"""
GENERAL FUNCTIONS
"""

#get current UTC date and time as text
def get_today():
    utc_t = datetime.utcnow()
    date_str = datemethod.strftime(utc_t, '%Y%m%dZ%H%M%S')
    return date_str

#save JSON file
def save_json(file_path, json_object):
    file_name = get_today() = '.json'
    full_path = os.path.join(file_path, file_name)
    with open(full_path, 'w') as json_file:
        json.dump(json_object, json_file)

#save object to pickle
def save_pickle(output_path, pickle_object, file_name):
    output_name = os.path.join(output_path, file_name + '.pkl')
    with open(output_name, 'wb') as pkl_object:
        pickle.dump(pickle_object, pkl_object)

#open pickle file
def open_pickle(pickle_path):
    with open(pickle_path, 'rb') as pickle_file:
        object_name = pickle.load(pickle_file, encoding='UTF-8')
    return object_name

#Get urls from news object
def open_file(news_object_path):
    news_object_file = os.path.join(news_object_path, 'news_dump_object.pkl')
    if os.path.isfile(news_object_file):
        old_news = open_pickle(news_object_file)
    else:
        old_news = []
    old_url_list = []
    for item in old_news:
        url = item['url']
        old_url_list.append(url)
    old_url_set = set(old_url_list)
    return old_news, old_url_set
    
#url check
def url_check(old_url_set, url):
    url_set = set([url])
    test_set = old_url_set & url_set
    if len(test_set) == 0:
        check = False
    else:
        check = True
    return check

#Concat outut lists
def concat_lists(out_lists):
    output = []
    for outlist in out_lists:
        output = output + outlist
    return output

"""
GET LINKS FROM HTML
"""

#get HTML with page scroll
def get_html_scroll(url):
    browser = webdriver.Firefox()
    browser.get(url)
    lenOfPage = browser.execute_script("window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;")
    match=False
    while(match==False):
        lastCount = lenOfPage
        time.sleep(4)
        lenOfPage = browser.execute_script("window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;")
        if lastCount==lenOfPage:
            time.sleep(4)
            match=True
    post_elms = browser.page_source
    return post_elms

#get HTML file
def get_html(url):
    page = requests.get(url)
    html_out = html.fromstring(page.content)
    text = page.text
    return html_out, text

#get soup
def get_soup(html_string):
    soup = BeautifulSoup(html_string, 'html.parser')
    return soup

#get links
def get_soup_links(soup):
    links = []
    for link in soup.find_all('a'):
        out_link = link.get('href')
        links.append(out_link)
    return links

"""
REUTERS FUNCTIONS
"""
#get article links
def get_articles_reuters(links, old_url_set):
    articles = []
    for link in links:
        try:
            if 'article' in link:
                if url_check(old_url_set, link) == False:
                    articles.append(link)
        except:
            continue
    return articles

#get article html
def get_html_reuters(articles):
    soup_list = []
    for article in articles:
        _, text = get_html(article)
        soup = get_soup(text)
        soup_list.append(soup)
    return soup_list

#date formatter
def format_date(date):
    date_dict = {'January':'1','February':'2','March':'3','April':'4','May':'5','June':'6','July':'7','August':'8','September':'9','October':'10','November':'11','December':'12'}
    split_date = date.split(' ')
    year = split_date[2]
    day_list = split_date[1].split(',')[0]
    day = day_list
    month = date_dict[split_date[0]]
    out_date = year + '-' + month + '-' + day
    return out_date

#get elements
def get_reuters_elements(soup_list, articles):
    out_list = []
    i = 0
    for article in soup_list:
        link = articles[i]
        i += 1
        try:
            article_body = article.find_all('div', {'class': 'StandardArticleBody_body'})
            article_headline = article.find_all('h1', {'class': 'ArticleHeader_headline'})
            article_date = article.find_all('div', {'class': 'ArticleHeader_date'})
            try:
                date_time = article_date[0].text.split(' / ')
                date_in = date_time[0]
                date = format_date(date_in)
                a_time = date_time[1][1:]
            except:
                date = article_date[0].text
                time = article_date[0].text
            headline = article_headline[0].text
            article_p = []
            for item in article_body:
                p_list = item.find_all('p')
                for p in p_list:
                    article_p.append(p.text)
            out_text = ' '.join(article_p)
            out_dict = dict([('date',date),('time',a_time),('source','www.reuters.com'),('Title',headline),('Text',out_text),('url',link)])
            out_list.append(out_dict)
        except:
            print('Unable to decode...skipping article...')
            continue
    return out_list

#Execute Reuters script
def reuters(old_url_set):
    print('Getting Reuters articles...')
    url = 'https://www.reuters.com/theWire'
    post_elms = get_html_scroll(url)
    soup = get_soup(post_elms)
    links = get_soup_links(soup)
    articles = get_articles_reuters(links, old_url_set)
    soup_list = get_html_reuters(articles)
    reuters_list = get_reuters_elements(soup_list, articles)
    return reuters_list

"""
MAIN SCRIPT
"""
def main():
    news_object_path = os.getcwd()
    output_path = os.getcwd()
    old_news, old_url_set = open_file(news_object_path)
    reuters_list = reuters(old_url_set)
    out_lists = [old_news, reuters_list]
    output_all = concat_lists(out_lists)
    print('Saving JSON...')
    today_ = get_today()
    news_object_name = os.path.join(news_object_path, today_ + 'json')
    save_json(news_object_name, reuters_list)
    print('Saving news object...')
    save_pickle(output_path, output_all, 'news_dump_object')
    return reuters_list


"""
EXECUTE SCRIPT
"""
if __name__ == '__main__':
    reuters_list = main()
